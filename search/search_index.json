{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Solr OCR Highlighting This Solr plugin lets you put word-level OCR text into one or more of you documents\u2019 fields and then allows you to obtain structured highlighting data with the text and its position on the page at query time: { \"page\": \"page_380\", \"text\": \"to those parts, subject to unreasonable claims from the pro\u00adprietor \" \"of Maryland, until the year 17C2, when the whole controversy was \" \"settled by Charles <em>Mason</em> and Jeremiah <em>Dixon</em>, upon \" \"their return from an observation of the tran\u00adsit of Venus, at the \" \"Cape of Good Hope, where they\", \"score\": 11110209, \"region\": { \"ulx\": 196, \"uly\": 1703, \"lrx\": 1232, \"lry\": 1968 }, \"highlights\": [ [{ \"text\": \"Mason\", \"ulx\": 675, \"uly\": 110, \"lrx\": 783, \"lry\": 141 }], [{ \"text\": \"Dixon,\", \"ulx\": 1, \"uly\": 167, \"lrx\": 119, \"lry\": 204 }] ] } All this can optionally be done without having to store any OCR text in the index itself: The plugin can lazy-load only the parts required for highlighting at query time from your original OCR input documents. It works by extending Solr\u2019s standard UnifiedHighlighter with support for loading external field values and determining OCR positions from those field values. This means that all options and query types supported by the UnifiedHighlighter are also supported for OCR highlighting. The plugin also works transparently with non-OCR fields and just lets the default implementation handle those. The plugin works with all Solr versions >= 7.x (tested with 7.6, 7.7 and 8.0). Features Index various OCR formats without little to no preprocessing hOCR ALTO MiniOCR Retrieve all the information needed to render a highlighted snippet view directly from Solr, without postprocessing Keeps your index size manageable by re-using OCR documents on disk for highlighting Getting Started To get started, refer to the Getting Started documentation .","title":"Solr OCR Highlighting"},{"location":"#solr-ocr-highlighting","text":"This Solr plugin lets you put word-level OCR text into one or more of you documents\u2019 fields and then allows you to obtain structured highlighting data with the text and its position on the page at query time: { \"page\": \"page_380\", \"text\": \"to those parts, subject to unreasonable claims from the pro\u00adprietor \" \"of Maryland, until the year 17C2, when the whole controversy was \" \"settled by Charles <em>Mason</em> and Jeremiah <em>Dixon</em>, upon \" \"their return from an observation of the tran\u00adsit of Venus, at the \" \"Cape of Good Hope, where they\", \"score\": 11110209, \"region\": { \"ulx\": 196, \"uly\": 1703, \"lrx\": 1232, \"lry\": 1968 }, \"highlights\": [ [{ \"text\": \"Mason\", \"ulx\": 675, \"uly\": 110, \"lrx\": 783, \"lry\": 141 }], [{ \"text\": \"Dixon,\", \"ulx\": 1, \"uly\": 167, \"lrx\": 119, \"lry\": 204 }] ] } All this can optionally be done without having to store any OCR text in the index itself: The plugin can lazy-load only the parts required for highlighting at query time from your original OCR input documents. It works by extending Solr\u2019s standard UnifiedHighlighter with support for loading external field values and determining OCR positions from those field values. This means that all options and query types supported by the UnifiedHighlighter are also supported for OCR highlighting. The plugin also works transparently with non-OCR fields and just lets the default implementation handle those. The plugin works with all Solr versions >= 7.x (tested with 7.6, 7.7 and 8.0).","title":"Solr OCR Highlighting"},{"location":"#features","text":"Index various OCR formats without little to no preprocessing hOCR ALTO MiniOCR Retrieve all the information needed to render a highlighted snippet view directly from Solr, without postprocessing Keeps your index size manageable by re-using OCR documents on disk for highlighting","title":"Features"},{"location":"#getting-started","text":"To get started, refer to the Getting Started documentation .","title":"Getting Started"},{"location":"example/","text":"The repository includes a full-fledged example setup based on the Google Books 1000 Dataset . It consists of 1000 Volumes along with their OCRed text in the hOCR format and all book pages as full resolution JPEG images. The example ships with a search interface that allows querying the OCRed texts and displays the matching passages as highlighted image and text snippets. Also included is a small IIIF-Viewer that allows viewing the complete volumes and searching for text within them. Online version A public instance of this example is available at https://ocrhl.jbaiter.de . The Solr server can be queried at https://ocrhl.jbaiter.de/solr , e.g. q=\"mason dixon\"~10\" Prerequisites To run the example setup yourself, you will need: Docker and docker-compose Python 3 ~8GiB of free storage Running the example cd example docker-compose up -d ./ingest_google100.py Access http://localhost:8181 in your browser Search Frontend IIIF Content Search Solr Configuration Walkthrough solrconfig.xml <!-- The Google 1000 books corpus used for the example is in the hOCR format --> <searchComponent class=\"org.mdz.search.solrocr.solr.HighlightComponent\" name=\"ocrHighlight\" ocrFormat=\"org.mdz.search.solrocr.formats.hocr.HocrFormat\"> <!-- We have a single field that contains OCR --> <lst name=\"ocrFields\"> <str>ocr_text</str> </lst> <!-- The example setup loads the ASCII-encoded OCR documents from local storage --> <fieldLoader class=\"org.mdz.search.solrocr.lucene.fieldloader.PathFieldLoader\" encoding=\"ascii\"> <lst name=\"externalFields\"> <!-- E.g. /google1000/Volume_0000.hocr --> <str name=\"ocr_text\">/google1000/{id}.hocr</str> </lst> </fieldLoader> </searchComponent> schema.xml <fieldtype name=\"text_ocr\" class=\"solr.TextField\" storeOffsetsWithPositions=\"true\" termVectors=\"true\"> <analyzer> <charFilter class=\"solr.HTMLStripCharFilterFactory\" /> <tokenizer class=\"solr.StandardTokenizerFactory\"/> <filter class=\"solr.LowerCaseFilterFactory\"/> <filter class=\"solr.StopFilterFactory\"/> <filter class=\"solr.PorterStemFilterFactory\"/> </analyzer> </fieldtype>","title":"Example Setup"},{"location":"example/#online-version","text":"A public instance of this example is available at https://ocrhl.jbaiter.de . The Solr server can be queried at https://ocrhl.jbaiter.de/solr , e.g. q=\"mason dixon\"~10\"","title":"Online version"},{"location":"example/#prerequisites","text":"To run the example setup yourself, you will need: Docker and docker-compose Python 3 ~8GiB of free storage","title":"Prerequisites"},{"location":"example/#running-the-example","text":"cd example docker-compose up -d ./ingest_google100.py Access http://localhost:8181 in your browser","title":"Running the example"},{"location":"example/#search-frontend","text":"","title":"Search Frontend"},{"location":"example/#iiif-content-search","text":"","title":"IIIF Content Search"},{"location":"example/#solr-configuration-walkthrough","text":"solrconfig.xml <!-- The Google 1000 books corpus used for the example is in the hOCR format --> <searchComponent class=\"org.mdz.search.solrocr.solr.HighlightComponent\" name=\"ocrHighlight\" ocrFormat=\"org.mdz.search.solrocr.formats.hocr.HocrFormat\"> <!-- We have a single field that contains OCR --> <lst name=\"ocrFields\"> <str>ocr_text</str> </lst> <!-- The example setup loads the ASCII-encoded OCR documents from local storage --> <fieldLoader class=\"org.mdz.search.solrocr.lucene.fieldloader.PathFieldLoader\" encoding=\"ascii\"> <lst name=\"externalFields\"> <!-- E.g. /google1000/Volume_0000.hocr --> <str name=\"ocr_text\">/google1000/{id}.hocr</str> </lst> </fieldLoader> </searchComponent> schema.xml <fieldtype name=\"text_ocr\" class=\"solr.TextField\" storeOffsetsWithPositions=\"true\" termVectors=\"true\"> <analyzer> <charFilter class=\"solr.HTMLStripCharFilterFactory\" /> <tokenizer class=\"solr.StandardTokenizerFactory\"/> <filter class=\"solr.LowerCaseFilterFactory\"/> <filter class=\"solr.StopFilterFactory\"/> <filter class=\"solr.PorterStemFilterFactory\"/> </analyzer> </fieldtype>","title":"Solr Configuration Walkthrough"},{"location":"external_storage/","text":"In the setup described in the Getting Started guide , the OCR documents are directly stored in Solr. This does not require any special configuration besides setting stored=true on the fields in the schema. However, this approach has several drawbacks, that might not make it practical for your use case: Index size: Raw OCR documents need to be stored in the Solr index, depending on the format these regularly reach sizes above 100MiB, which adds up really quickly with thousands of documents Memory Usage: During highlighting, Solr will load the full contents of the stored field into memory. Here again, the document sizes involved can make this problematic. Storing OCR outside of Solr To work around the above issues, the plugin can also access OCR documents stored outside of Solr for highlighting. This works by lazily loading only the parts of the documents that actually need to be highlighted at query time from an external source like the file system. This works, since with an appropriately configured index, Solr stores the character offsets of the indexed terms in the input document in the index. Given these offsets, we can seek into the input document and retrieve just the parts that are needed. There is, however, one complication: Internally, Solr works with UTF-16 strings, where every character takes up exactly 2 bytes, which is also what the stored character offsets are based on. In a UTF-16 encoded document, given a character offset of 32 , you can obtain the character at this position by simply seeking into the input document at position 64 (two bytes per character) and reading the two bytes that make up the character This would easy, but almost nobody stores documents in UTF-16, since it is very space-inefficient. Most real-world documents will be stored in UTF-8, which is a variable length encoding , meaning that the number of bytes a character takes up depends on the character itself (e.g. a is one byte, while \u00e4 is two bytes). This makes it impossible to calculate the byte offset of a given character, given only the character offset. To get around this limitation, you have two options: Encode the input documents in ASCII (= always one byte per character) and format non-ASCII codepoints as XML character entities (i.e. \u017f becomes &#383; ) Leave the input documents in UTF-8 and store the byte offset for every term in the Solr index Option #1 is the recommended way : It only requires a slight modification to your input documents with no effects on other consumers and offer the best experience. Document resolving To enable accessing external documents during highlighting, you will have to configure a FieldLoader in the <searchComponent> section of your solrconfig.xml . For documents on the local filesystem, the loader to use is org.mdz.search.solrocr.lucene.fieldloader.PathFieldLoader . <searchComponent class=\"org.mdz.search.solrocr.solr.HighlightComponent\" name=\"highlight\" ocrFormat=\"org.mdz.search.solrocr.formats.alto.AltoFormat\"> <lst name=\"ocrFields\"> <str>ocr_text</str> </lst> <fieldLoader class=\"org.mdz.search.solrocr.lucene.fieldloader.PathFieldLoader\" encoding=\"ascii\"> <lst name=\"externalFields\"> <str name=\"ocr_text\">/google1000/{id}/hOCR.html</str> </lst> </fieldLoader> </searchComponent> To tell the loader where to find the file for a given document, you have to provide it with the encoding of the files ( ascii or utf8 ) and a mapping from the field name to a path pattern . The path pattern can include arbitrary field names enclosed in curly braces. During resolving, these will be replaced with the contents of the field for the current document. The OCR document will then be loaded from the path described by the resulting string. For example, given a document where the id field is set to 1337 , the pattern /data/ocr/{id}_ocr.xml would load the OCR document from /data/ocr/1337_ocr.xml . You can use any field that is defined in your schema inside of the pattern. You can also use a Python-like slicing syntax to only refer to parts of the field values (e.g. {id[:10]} , {id[2:4]} , {id[:-5]} ). Caution When using external documents for highlighting, the performance depends almost exclusively on how fast the underlying storage is able to perform random I/O. This is why using flash storage for the documents is highly recommended . Note If your use case requires more sophisticated resolving, or you want to load the document contents from sources other than the file system, the plugin provides an ExternalFieldLoader interface that you can implement and just pass in the class parameter. Indexing multiple files as a single document In some environments, OCR documents are stored with one file per page in the OCRed document. The plugin supports indexing these files as a single document if some pre-conditions are met: The lexicographical order of the file paths that make up the pages of a document is equal to the order of those pages in the document The files are concatenated in the same order without any characters in between at indexing time If these conditions are met, add the multiple=\"true\" option to your field loader configuration and a wildcard to your path patterns (e.g. /local/ocr/{id}/*.xml ). At indexing time, instead of POSTing the contents of a single file in your document\u2019s OCR field, you submit the concatenated contents of all files (or, in the case of UTF-8 documents , the index for the concatenated contents). Refer to the unit test to see an example setup for this use case. Encoding: ASCII with XML-Escapes To use ASCII-encoded documents, the only thing you have to do is pass ascii in the encoding parameter of your fieldLoader and make sure that they do not contain any unescaped UTF-8 codepoints . This can be done very effectively with this little Python snippet: utftext.encode('ascii', 'xmlcharrefreplace') The repository also includes a script that you can use to do the conversion: $ cat input.xml | ./ascii_escape > output.xml #or: $ ./ascii_escape input.xml output.xml Encoding: UTF-8 Caution This approach comes with significant drawbacks: You cannot use the modern highlighting approach ( hl.weightMatches=true , default in Solr >=8) Your index will be significantly larger than with the ASCII-approach. In order for the plugin to know which parts of the input files to read during highlighting, you need to tell it the byte offset for every token in the input document. Solr will then store this offset in the term payloads and the plugin will load it from there using highlighting. This approach won\u2019t allow directly indexing the OCR documents, you will have to convert them to a special format that includes the byte offset for every token, e.g. Die\u017fe\u26915107923 leuchtenden\u26915108028 treuherzigen\u26915108138 blauen\u26915108250 Augen,\u26915108357 . The \u2691 character is the default delimiter that separates the term from its offset, but you can pick your own value with the included tool and the delimiter attribute in the configuration below. Note This format will only be used for indexing, you don\u2019t need to store it on disk. For highlighting, your unmodified OCR documents will be used. You don\u2019t have to do this by yourself, we provide a Java implementation for every supported format ( hOCR , ALTO and MiniOCR ) as well as a cross-platform command-line tool (available on the GitHub releases page ): # The tool works transparently with all supported formats ./offsets-parser ../example/google1000/Volume_0000.hocr > out.txt #or, for multiple files that should be indexed as one document: $ cat page1.xml page2.xml | ./offsets-parser > out.txt In your schema, you will have to enable term positions, term vectors and term payloads. In your indexing analyzer chain, you need to remove the HTMLStripCharFilterFactory (and the AltoCharFilterFactory if using ALTO) and instead tell Solr to tokenize on whitespace, split off those offsets during indexing and store them as payloads. <!-- Positions, Term Vectors and Term Payloads are mandatory with this approach. --> <fieldtype name=\"text_ocr\" class=\"solr.TextField\" termPositions=\"true\" termVectors=\"true\" termPayloads=\"true\"> <analyzer> <!-- Mandatory, the input is a whitespace-separated sequence of {term}{delimiter}{offset} units and has to be split on whitespace --> <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/> <!-- The delimiter can be any UTF-16 codepoint, \"\u2691\" is used by default in the provided Java implementation and CLI tool --> <filter class=\"solr.DelimitedPayloadTokenFilterFactory\" delimiter=\"\u2691\" encoder=\"org.mdz.search.solrocr.lucene.byteoffset.ByteOffsetEncoder\" /> <!-- The WhitespaceTokenizer is really rudimentary. This filter will trim non-letter from the beginning/end, making the terms more similar to what you'd get with the StandardTokenizer. --> <filter class=\"org.mdz.search.solrocr.lucene.NonAlphaTrimFilterFactory\" /> <!-- rest of your analyzer chain --> <!-- ..... --> </analyzer> </fieldtype>","title":"External Storage"},{"location":"external_storage/#storing-ocr-outside-of-solr","text":"To work around the above issues, the plugin can also access OCR documents stored outside of Solr for highlighting. This works by lazily loading only the parts of the documents that actually need to be highlighted at query time from an external source like the file system. This works, since with an appropriately configured index, Solr stores the character offsets of the indexed terms in the input document in the index. Given these offsets, we can seek into the input document and retrieve just the parts that are needed. There is, however, one complication: Internally, Solr works with UTF-16 strings, where every character takes up exactly 2 bytes, which is also what the stored character offsets are based on. In a UTF-16 encoded document, given a character offset of 32 , you can obtain the character at this position by simply seeking into the input document at position 64 (two bytes per character) and reading the two bytes that make up the character This would easy, but almost nobody stores documents in UTF-16, since it is very space-inefficient. Most real-world documents will be stored in UTF-8, which is a variable length encoding , meaning that the number of bytes a character takes up depends on the character itself (e.g. a is one byte, while \u00e4 is two bytes). This makes it impossible to calculate the byte offset of a given character, given only the character offset. To get around this limitation, you have two options: Encode the input documents in ASCII (= always one byte per character) and format non-ASCII codepoints as XML character entities (i.e. \u017f becomes &#383; ) Leave the input documents in UTF-8 and store the byte offset for every term in the Solr index Option #1 is the recommended way : It only requires a slight modification to your input documents with no effects on other consumers and offer the best experience.","title":"Storing OCR outside of Solr"},{"location":"external_storage/#document-resolving","text":"To enable accessing external documents during highlighting, you will have to configure a FieldLoader in the <searchComponent> section of your solrconfig.xml . For documents on the local filesystem, the loader to use is org.mdz.search.solrocr.lucene.fieldloader.PathFieldLoader . <searchComponent class=\"org.mdz.search.solrocr.solr.HighlightComponent\" name=\"highlight\" ocrFormat=\"org.mdz.search.solrocr.formats.alto.AltoFormat\"> <lst name=\"ocrFields\"> <str>ocr_text</str> </lst> <fieldLoader class=\"org.mdz.search.solrocr.lucene.fieldloader.PathFieldLoader\" encoding=\"ascii\"> <lst name=\"externalFields\"> <str name=\"ocr_text\">/google1000/{id}/hOCR.html</str> </lst> </fieldLoader> </searchComponent> To tell the loader where to find the file for a given document, you have to provide it with the encoding of the files ( ascii or utf8 ) and a mapping from the field name to a path pattern . The path pattern can include arbitrary field names enclosed in curly braces. During resolving, these will be replaced with the contents of the field for the current document. The OCR document will then be loaded from the path described by the resulting string. For example, given a document where the id field is set to 1337 , the pattern /data/ocr/{id}_ocr.xml would load the OCR document from /data/ocr/1337_ocr.xml . You can use any field that is defined in your schema inside of the pattern. You can also use a Python-like slicing syntax to only refer to parts of the field values (e.g. {id[:10]} , {id[2:4]} , {id[:-5]} ). Caution When using external documents for highlighting, the performance depends almost exclusively on how fast the underlying storage is able to perform random I/O. This is why using flash storage for the documents is highly recommended . Note If your use case requires more sophisticated resolving, or you want to load the document contents from sources other than the file system, the plugin provides an ExternalFieldLoader interface that you can implement and just pass in the class parameter.","title":"Document resolving"},{"location":"external_storage/#indexing-multiple-files-as-a-single-document","text":"In some environments, OCR documents are stored with one file per page in the OCRed document. The plugin supports indexing these files as a single document if some pre-conditions are met: The lexicographical order of the file paths that make up the pages of a document is equal to the order of those pages in the document The files are concatenated in the same order without any characters in between at indexing time If these conditions are met, add the multiple=\"true\" option to your field loader configuration and a wildcard to your path patterns (e.g. /local/ocr/{id}/*.xml ). At indexing time, instead of POSTing the contents of a single file in your document\u2019s OCR field, you submit the concatenated contents of all files (or, in the case of UTF-8 documents , the index for the concatenated contents). Refer to the unit test to see an example setup for this use case.","title":"Indexing multiple files as a single document"},{"location":"external_storage/#ascii","text":"To use ASCII-encoded documents, the only thing you have to do is pass ascii in the encoding parameter of your fieldLoader and make sure that they do not contain any unescaped UTF-8 codepoints . This can be done very effectively with this little Python snippet: utftext.encode('ascii', 'xmlcharrefreplace') The repository also includes a script that you can use to do the conversion: $ cat input.xml | ./ascii_escape > output.xml #or: $ ./ascii_escape input.xml output.xml","title":"Encoding: ASCII with XML-Escapes"},{"location":"external_storage/#utf8","text":"Caution This approach comes with significant drawbacks: You cannot use the modern highlighting approach ( hl.weightMatches=true , default in Solr >=8) Your index will be significantly larger than with the ASCII-approach. In order for the plugin to know which parts of the input files to read during highlighting, you need to tell it the byte offset for every token in the input document. Solr will then store this offset in the term payloads and the plugin will load it from there using highlighting. This approach won\u2019t allow directly indexing the OCR documents, you will have to convert them to a special format that includes the byte offset for every token, e.g. Die\u017fe\u26915107923 leuchtenden\u26915108028 treuherzigen\u26915108138 blauen\u26915108250 Augen,\u26915108357 . The \u2691 character is the default delimiter that separates the term from its offset, but you can pick your own value with the included tool and the delimiter attribute in the configuration below. Note This format will only be used for indexing, you don\u2019t need to store it on disk. For highlighting, your unmodified OCR documents will be used. You don\u2019t have to do this by yourself, we provide a Java implementation for every supported format ( hOCR , ALTO and MiniOCR ) as well as a cross-platform command-line tool (available on the GitHub releases page ): # The tool works transparently with all supported formats ./offsets-parser ../example/google1000/Volume_0000.hocr > out.txt #or, for multiple files that should be indexed as one document: $ cat page1.xml page2.xml | ./offsets-parser > out.txt In your schema, you will have to enable term positions, term vectors and term payloads. In your indexing analyzer chain, you need to remove the HTMLStripCharFilterFactory (and the AltoCharFilterFactory if using ALTO) and instead tell Solr to tokenize on whitespace, split off those offsets during indexing and store them as payloads. <!-- Positions, Term Vectors and Term Payloads are mandatory with this approach. --> <fieldtype name=\"text_ocr\" class=\"solr.TextField\" termPositions=\"true\" termVectors=\"true\" termPayloads=\"true\"> <analyzer> <!-- Mandatory, the input is a whitespace-separated sequence of {term}{delimiter}{offset} units and has to be split on whitespace --> <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/> <!-- The delimiter can be any UTF-16 codepoint, \"\u2691\" is used by default in the provided Java implementation and CLI tool --> <filter class=\"solr.DelimitedPayloadTokenFilterFactory\" delimiter=\"\u2691\" encoder=\"org.mdz.search.solrocr.lucene.byteoffset.ByteOffsetEncoder\" /> <!-- The WhitespaceTokenizer is really rudimentary. This filter will trim non-letter from the beginning/end, making the terms more similar to what you'd get with the StandardTokenizer. --> <filter class=\"org.mdz.search.solrocr.lucene.NonAlphaTrimFilterFactory\" /> <!-- rest of your analyzer chain --> <!-- ..... --> </analyzer> </fieldtype>","title":"Encoding: UTF-8"},{"location":"faq/","text":"Can I have a document that points to a part of an OCR document on external storage? Yes, with an ugly hack . This use case appears e.g. when indexing digital newspapers, where you have a single large volume on disk (e.g. the OCR text for the bound volume containing all issues from the year 1878) but you want your Solr documents to be more fine-grained (e.g. on the issue or even article level). A problem in Solr is that the source of the offsets that are used for highlighting are always relative to the actual document that was used for indexing and cannot be easily customized. To work around this: Replace all of the content preceding your sub-section with a single XML comment tag that is exactly as long as the old content and discard all content that follows after the sub-section (We told you the solution was hacky, didn\u2019t we?). This will lead the analyzer chain to discard all of the undesired content, while still storing the correct offsets for the sub-section content in the index. Note Since the plugin doesn\u2019t do any actual XML parsing, the masked documents don\u2019t have to be valid XML. Minimal example before masking: <l>Some content that you don't want in your Solr document</l> <l>Here's the content you want in the index for this document</l> <l>And here's some extra content following it that you don't want</l> Minimal example after masking: <!----------------------------------------------------------> <l>Here's the content you want in the index for this document</l> Note This technique is also very useful if you want to exclude certain parts of the OCR document from the index. It can be very helpful to filter out header and footer lines or page numbers from the search index, e.g. to properly support search hits that cross page boundaries.","title":"Frequently Asked Questions"},{"location":"faq/#partial-docs","text":"Yes, with an ugly hack . This use case appears e.g. when indexing digital newspapers, where you have a single large volume on disk (e.g. the OCR text for the bound volume containing all issues from the year 1878) but you want your Solr documents to be more fine-grained (e.g. on the issue or even article level). A problem in Solr is that the source of the offsets that are used for highlighting are always relative to the actual document that was used for indexing and cannot be easily customized. To work around this: Replace all of the content preceding your sub-section with a single XML comment tag that is exactly as long as the old content and discard all content that follows after the sub-section (We told you the solution was hacky, didn\u2019t we?). This will lead the analyzer chain to discard all of the undesired content, while still storing the correct offsets for the sub-section content in the index. Note Since the plugin doesn\u2019t do any actual XML parsing, the masked documents don\u2019t have to be valid XML. Minimal example before masking: <l>Some content that you don't want in your Solr document</l> <l>Here's the content you want in the index for this document</l> <l>And here's some extra content following it that you don't want</l> Minimal example after masking: <!----------------------------------------------------------> <l>Here's the content you want in the index for this document</l> Note This technique is also very useful if you want to exclude certain parts of the OCR document from the index. It can be very helpful to filter out header and footer lines or page numbers from the search index, e.g. to properly support search hits that cross page boundaries.","title":"Can I have a document that points to a part of an OCR document on external storage?"},{"location":"formats/","text":"In general the plugin assumes that all OCR formats encode their documents in a hierarchy of blocks . For all supported formats, we map their block types to these general types: Page : optional if there is only a single page in a document Block : optional if hl.ocr.limitBlock is set to a different value at query time Section : optional Paragraph : optional Line : (optional if hl.ocr.contextBlock is set to a different value at query time) Word : required These block types can be used in the hl.ocr.limitBlock and hl.ocr.contextBlock query parameters to control how snippets are generated. hOCR In the Solr configuration , set the ocrFormat attribute on the <searchComponent /> to org.mdz.search.solrocr.formats.hocr.HocrFormat . In the Schema , make sure that solr.HTMLStripCharFilterFactory is the first filter in your indexing analyzer chain for OCR fields. Block type mapping: Block hOCR class notes Word ocrx_word needs to have a bbox attribute with the coordinates on the page Page ocr_page needs to have a ppageno attribute with a page identifier Block ocr_carea / ocrx_block Section ocr_chapter / ocr_section / ocr_subsection / ocr_subsubsection Paragraph ocr_par Line ocr_line or ocrx_line ALTO In the Solr configuration , set the ocrFormat attribute on the <searchComponent /> to org.mdz.search.solrocr.formats.alto.AltoFormat . In the Schema , make sure that org.mdz.search.solrocr.formats.alto.AltoCharFilterFactory is the first filter in your indexing analyzer chain for OCR fields. Block type mapping: Block ALTO tag notes Word <String /> needs to have CONTENT , HPOS , VPOS , WIDTH and HEIGHT attributes Line <TextLine /> Block <TextBlock /> Page <Page /> needs to have an ID attribute with a page identifier Section not mapped Paragraph not mapped MiniOCR This plugin also includes support for a custom non-standard OCR format that we dubbed MiniOCR . This format is intended for use cases where reusing the existing OCR files is not possible (e.g. because they\u2019re in an unsupported format or because you don\u2019t want to ASCII-encode them and still use the modern highlighting approach). In these cases, minimizing the storage requirements for the derived OCR files is important, which is why we defined this minimalistic format. A basic example looks like this: <p id=\"page_identifier\"> <b> <l><w x=\"50 50 100 100\">A</w> <w x=\"150 50 100 100\">Line</w></l> </b> </p> In the Solr configuration , set the ocrFormat attribute on the <searchComponent /> to org.mdz.search.solrocr.formats.mini.MiniOcrFormat . In the Schema , make sure that solr.HTMLStripCharFilterFactory is the first filter in your indexing analyzer chain for OCR fields. Block type mapping: Block MiniOCR tag notes Word <w/> needs to have box attribute with {x} {y} {width} {height} . Values can be integers or floats between 0 and 1 Line <l/> Block <b/> Page <p/> needs to have an id attribute with a page identifier Section not mapped Paragraph not mapped","title":"OCR Formats"},{"location":"formats/#hocr","text":"In the Solr configuration , set the ocrFormat attribute on the <searchComponent /> to org.mdz.search.solrocr.formats.hocr.HocrFormat . In the Schema , make sure that solr.HTMLStripCharFilterFactory is the first filter in your indexing analyzer chain for OCR fields. Block type mapping: Block hOCR class notes Word ocrx_word needs to have a bbox attribute with the coordinates on the page Page ocr_page needs to have a ppageno attribute with a page identifier Block ocr_carea / ocrx_block Section ocr_chapter / ocr_section / ocr_subsection / ocr_subsubsection Paragraph ocr_par Line ocr_line or ocrx_line","title":"hOCR"},{"location":"formats/#alto","text":"In the Solr configuration , set the ocrFormat attribute on the <searchComponent /> to org.mdz.search.solrocr.formats.alto.AltoFormat . In the Schema , make sure that org.mdz.search.solrocr.formats.alto.AltoCharFilterFactory is the first filter in your indexing analyzer chain for OCR fields. Block type mapping: Block ALTO tag notes Word <String /> needs to have CONTENT , HPOS , VPOS , WIDTH and HEIGHT attributes Line <TextLine /> Block <TextBlock /> Page <Page /> needs to have an ID attribute with a page identifier Section not mapped Paragraph not mapped","title":"ALTO"},{"location":"formats/#miniocr","text":"This plugin also includes support for a custom non-standard OCR format that we dubbed MiniOCR . This format is intended for use cases where reusing the existing OCR files is not possible (e.g. because they\u2019re in an unsupported format or because you don\u2019t want to ASCII-encode them and still use the modern highlighting approach). In these cases, minimizing the storage requirements for the derived OCR files is important, which is why we defined this minimalistic format. A basic example looks like this: <p id=\"page_identifier\"> <b> <l><w x=\"50 50 100 100\">A</w> <w x=\"150 50 100 100\">Line</w></l> </b> </p> In the Solr configuration , set the ocrFormat attribute on the <searchComponent /> to org.mdz.search.solrocr.formats.mini.MiniOcrFormat . In the Schema , make sure that solr.HTMLStripCharFilterFactory is the first filter in your indexing analyzer chain for OCR fields. Block type mapping: Block MiniOCR tag notes Word <w/> needs to have box attribute with {x} {y} {width} {height} . Values can be integers or floats between 0 and 1 Line <l/> Block <b/> Page <p/> needs to have an id attribute with a page identifier Section not mapped Paragraph not mapped","title":"MiniOCR"},{"location":"getting_started/","text":"Requirements Some familiarity with configuring Solr Solr >= 7.x OCR documents need to be in hOCR , ALTO or MiniOCR formats, with at least page-, and word-level segmentation One OCR file needs to correspond to one document in the search index There is a hack to support multiple index documents per file , but the reverse (multiple files per index document) is not possible, you will have to merge the files. Installation Download the JAR for the latest release from the GitHub Releases website and drop it into your core\u2019s lib directory. Compiling If you want to use the latest bleeding-edge version, you can also compile the plugin yourself. For this you will need at least Java 8 and Maven: $ mvn package The JAR will be in target/solr-ocrhighlighting-$version.jar . Configuration Note If you\u2019re using a format other than hOCR, refer to the OCR Formats documentation for the values you need to put into your configuration and schema. Using the plugin requires some configuration in your solrconfig.xml . For one, you have to define a search component to add the OCR highlighting information for your format to the response. Also, you will have to specify which of your fields contain OCR text (i.e. the solrconfig is currently tied to the schema): <config> <searchComponent class=\"org.mdz.search.solrocr.solr.HighlightComponent\" name=\"ocrHighlight\" ocrFormat=\"org.mdz.search.solrocr.formats.hocr.HocrFormat\"> <lst name=\"ocrFields\"> <!-- Requires a field named `ocr_text` in the schema --> <str>ocr_text</str> </lst> </searchComponent> <!-- Add the OCR highlighting component to the components on your request handler(s) --> <requestHandler name=\"/select\" class=\"solr.SearchHandler\"> <arr name=\"components\"> <str>query</str> <str>highlight</str> <!-- This value needs to be equal to the `name` attribute on the searchComponent --> <str>ocrHighlight</str> </arr> </requestHandler> </config> Note For this simple guide, the configuration will store the OCR documents in the Solr index itself . If you want to store them outside of the index on the file system, refer to the External Storage documentation . For your schema, you will have to define a type that enables the storage of offsets and positions. Enabling term vectors is optional, although it significantly speeds up highlights wildcard queries. The indexing analyzer chain for the field type needs to start with the HTMLStripCharFilterFactory . For ALTO, you need the specialized org.mdz.search.solrocr.formats.alto.AltoCharFilterFactory instead. <fieldtype name=\"text_ocr\" class=\"solr.TextField\" storeOffsetsWithPositions=\"true\" termVectors=\"true\"> <analyzer> <!-- Strip away the XML/HTML tags to arrive at a plaintext version of the OCR --> <charFilter class=\"solr.HTMLStripCharFilterFactory\" /> <!-- rest of your analyzer chain --> <!-- ..... --> </analyzer> </fieldtype> You can then use this new field type in your schema: <fields> <field name=\"id\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\" required=\"true\"/> <field name=\"ocr_text\" type=\"text_ocr\" indexed=\"true\" stored=\"true\" /> </fields> After you\u2019ve saved your configuration and schema and restarted Solr, you can verify that the plugin is activated by checking the Plugins / Stats section in the Solr admin interface for your core. There should now be a Highlighting plugin with the name you chose: Indexing Indexing for most usage scenarios (except for externally stored UTF8 files ) is simple: Just POST the document with appropriately escaped OCR in the corresponding field. A simple example using curl and jq for escaping: $ curl -H \"Content-Type: application/json\" \\ -d '{\"id\": \"document_id\", \"ocr_field\": $(cat ocrdocument.xml |jq -Rs)}' That\u2019s it! Assuming Solr is correctly configured, you should now have a document that can be highlighted in your index. Querying At query time, no extra parameters besides hl=true and an inclusion of your OCR fields in the hl.fields parameter are required. Note For a list of all available parameters, rever to the Query Parameters documentation Given an index with the fields id , title and ocr_text , and a query like the following: GET http://localhost:8983/solr/ocrtest/select?fl=id,title&hl=on&hl.fl=ocr_text&df=ocr_text&q=pirate should yield a response like this: { \"responseHeader\": { \"status\": 0, \"QTime\": 182 }, \"response\": { \"numFound\": 216, \"start\": 0, \"docs\": [ { \"id\": \"Volume_0282\", \"title\": \"Spartacus, Or, The Roman Gladiator\", } ] }, \"highlighting\": { \"Volume_0282\": {} }, \"ocrHighlighting\": { \"Volume_0282\": { \"ocr_text\": { \"snippets\": [ { \"page\": \"page_44\", \"text\": \"Easy to have, impregnable to hold, An Island to be rul'd, and \" \"I to rule ![Enter a Gladiator, shouting towards <em>Pirates</em>' \" \"quarter.]GLAD. Ho, there ! what ho ! the troops are on the move, \" \"My Chief! my Chief! the <em>Pirates</em> steal away !\", \"score\": 881062.75, \"region\": { \"ulx\": 194, \"uly\": 807, \"lrx\": 1196, \"lry\": 1008 }, \"highlights\": [ [ { \"text\": \"Pirates'\", \"ulx\": 694, \"uly\": 82, \"lrx\": 823, \"lry\": 111 } ], [ { \"text\": \"Pirates\", \"ulx\": 450, \"uly\": 162, \"lrx\": 563, \"lry\": 190 } ] ] } ], \"numTotal\": 11 } } } } As you can see, the ocrHighlighting component contains for every field in every matching document a list of snippets that match the query. The snippet contains the page the match occurred on, the matching text, the score of the passage and the coordinates of the region on the page image containing the passage text. Additionally, it also includes the region and text for every highlighting hit (i.e. the actual tokens that matched the query). Every hit in the document is an array of hit regions: If a given hit spans multiple lines, it is split into multiple regions, one for every line. Note that the hit region coordinates are by default relative to the containing region, not the page! (although this can be changed ). numTotal lists the total number of snippets that were found in this document for the given search query. You can control the number of snippets to be returned with the hl.snippets parameter (or f.{fieldname}.hl.snippets if you only want to set it for a given field). Snippets are sorted by their score .","title":"Getting Started"},{"location":"getting_started/#requirements","text":"Some familiarity with configuring Solr Solr >= 7.x OCR documents need to be in hOCR , ALTO or MiniOCR formats, with at least page-, and word-level segmentation One OCR file needs to correspond to one document in the search index There is a hack to support multiple index documents per file , but the reverse (multiple files per index document) is not possible, you will have to merge the files.","title":"Requirements"},{"location":"getting_started/#installation","text":"Download the JAR for the latest release from the GitHub Releases website and drop it into your core\u2019s lib directory.","title":"Installation"},{"location":"getting_started/#compiling","text":"If you want to use the latest bleeding-edge version, you can also compile the plugin yourself. For this you will need at least Java 8 and Maven: $ mvn package The JAR will be in target/solr-ocrhighlighting-$version.jar .","title":"Compiling"},{"location":"getting_started/#configuration","text":"Note If you\u2019re using a format other than hOCR, refer to the OCR Formats documentation for the values you need to put into your configuration and schema. Using the plugin requires some configuration in your solrconfig.xml . For one, you have to define a search component to add the OCR highlighting information for your format to the response. Also, you will have to specify which of your fields contain OCR text (i.e. the solrconfig is currently tied to the schema): <config> <searchComponent class=\"org.mdz.search.solrocr.solr.HighlightComponent\" name=\"ocrHighlight\" ocrFormat=\"org.mdz.search.solrocr.formats.hocr.HocrFormat\"> <lst name=\"ocrFields\"> <!-- Requires a field named `ocr_text` in the schema --> <str>ocr_text</str> </lst> </searchComponent> <!-- Add the OCR highlighting component to the components on your request handler(s) --> <requestHandler name=\"/select\" class=\"solr.SearchHandler\"> <arr name=\"components\"> <str>query</str> <str>highlight</str> <!-- This value needs to be equal to the `name` attribute on the searchComponent --> <str>ocrHighlight</str> </arr> </requestHandler> </config> Note For this simple guide, the configuration will store the OCR documents in the Solr index itself . If you want to store them outside of the index on the file system, refer to the External Storage documentation . For your schema, you will have to define a type that enables the storage of offsets and positions. Enabling term vectors is optional, although it significantly speeds up highlights wildcard queries. The indexing analyzer chain for the field type needs to start with the HTMLStripCharFilterFactory . For ALTO, you need the specialized org.mdz.search.solrocr.formats.alto.AltoCharFilterFactory instead. <fieldtype name=\"text_ocr\" class=\"solr.TextField\" storeOffsetsWithPositions=\"true\" termVectors=\"true\"> <analyzer> <!-- Strip away the XML/HTML tags to arrive at a plaintext version of the OCR --> <charFilter class=\"solr.HTMLStripCharFilterFactory\" /> <!-- rest of your analyzer chain --> <!-- ..... --> </analyzer> </fieldtype> You can then use this new field type in your schema: <fields> <field name=\"id\" type=\"string\" multiValued=\"false\" indexed=\"true\" stored=\"true\" required=\"true\"/> <field name=\"ocr_text\" type=\"text_ocr\" indexed=\"true\" stored=\"true\" /> </fields> After you\u2019ve saved your configuration and schema and restarted Solr, you can verify that the plugin is activated by checking the Plugins / Stats section in the Solr admin interface for your core. There should now be a Highlighting plugin with the name you chose:","title":"Configuration"},{"location":"getting_started/#indexing","text":"Indexing for most usage scenarios (except for externally stored UTF8 files ) is simple: Just POST the document with appropriately escaped OCR in the corresponding field. A simple example using curl and jq for escaping: $ curl -H \"Content-Type: application/json\" \\ -d '{\"id\": \"document_id\", \"ocr_field\": $(cat ocrdocument.xml |jq -Rs)}' That\u2019s it! Assuming Solr is correctly configured, you should now have a document that can be highlighted in your index.","title":"Indexing"},{"location":"getting_started/#querying","text":"At query time, no extra parameters besides hl=true and an inclusion of your OCR fields in the hl.fields parameter are required. Note For a list of all available parameters, rever to the Query Parameters documentation Given an index with the fields id , title and ocr_text , and a query like the following: GET http://localhost:8983/solr/ocrtest/select?fl=id,title&hl=on&hl.fl=ocr_text&df=ocr_text&q=pirate should yield a response like this: { \"responseHeader\": { \"status\": 0, \"QTime\": 182 }, \"response\": { \"numFound\": 216, \"start\": 0, \"docs\": [ { \"id\": \"Volume_0282\", \"title\": \"Spartacus, Or, The Roman Gladiator\", } ] }, \"highlighting\": { \"Volume_0282\": {} }, \"ocrHighlighting\": { \"Volume_0282\": { \"ocr_text\": { \"snippets\": [ { \"page\": \"page_44\", \"text\": \"Easy to have, impregnable to hold, An Island to be rul'd, and \" \"I to rule ![Enter a Gladiator, shouting towards <em>Pirates</em>' \" \"quarter.]GLAD. Ho, there ! what ho ! the troops are on the move, \" \"My Chief! my Chief! the <em>Pirates</em> steal away !\", \"score\": 881062.75, \"region\": { \"ulx\": 194, \"uly\": 807, \"lrx\": 1196, \"lry\": 1008 }, \"highlights\": [ [ { \"text\": \"Pirates'\", \"ulx\": 694, \"uly\": 82, \"lrx\": 823, \"lry\": 111 } ], [ { \"text\": \"Pirates\", \"ulx\": 450, \"uly\": 162, \"lrx\": 563, \"lry\": 190 } ] ] } ], \"numTotal\": 11 } } } } As you can see, the ocrHighlighting component contains for every field in every matching document a list of snippets that match the query. The snippet contains the page the match occurred on, the matching text, the score of the passage and the coordinates of the region on the page image containing the passage text. Additionally, it also includes the region and text for every highlighting hit (i.e. the actual tokens that matched the query). Every hit in the document is an array of hit regions: If a given hit spans multiple lines, it is split into multiple regions, one for every line. Note that the hit region coordinates are by default relative to the containing region, not the page! (although this can be changed ). numTotal lists the total number of snippets that were found in this document for the given search query. You can control the number of snippets to be returned with the hl.snippets parameter (or f.{fieldname}.hl.snippets if you only want to set it for a given field). Snippets are sorted by their score .","title":"Querying"},{"location":"queryparams/","text":"You can customize the way the passages are formed. By default the passage will include two lines above and below the line with the match. Passages will also not cross block boundaries (what this translates to depends on the format). These parameters can be changed at query time: hl.ocr.contextBlock : Select which block type should be considered for determining the context. Valid values are word , line , paragraph , block or page and defaults to line . hl.ocr.contextSize : Set the number of blocks above and below the matching block to be included in the passage. Defaults to 2 . hl.ocr.limitBlock : Set the block type that passages may not exceed. Valid values are none word , line , paragraph , block or page . This value defaults to block , which means that snippets crossing page boundaries are disabled by default . Set the value to none if you want to enable this feature. hl.ocr.pageId : Only show passages from the page with this identifier. Useful if you want to implement a \u201cSearch on this page\u201d feature (e.g. for the IIIF Content Search API ). hl.ocr.absoluteHighlights : Return the coordinates of highlighted regions as absolute coordinates (i.e. relative to the page, not the snippet region)","title":"Query Parameters"}]}